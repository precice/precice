#!/usr/bin/python

# Import the fastest jsons library available
try:
    import ujson as json
except ImportError:
    try:
        import simplejson as json
    except ImportError:
        import json

import csv
import sys
import datetime
import argparse
import os
from collections import namedtuple
import functools


def mergedDict(dict1, dict2):
    merged = dict1.copy()
    merged.update(dict2)
    return merged

def requireModules(modules):
    import importlib
    notFound = [m for m in modules if importlib.util.find_spec(m) is None]
    if (notFound):
        def concat(l):
            if len(l) == 1:
                return l[0]
            else:
                return ", ".join(l[:-1]) + f", and {l[-1]}"
        print(
            f"This command requires the following additional dependencies: {concat(modules)}")
        print(f"The following is/are missing: {concat(notFound)}")
        sys.exit(1)


def loadRobust(content):
    try:
        return json.loads(content)
    except:
        print("Damaged input detected")
        content += "]}"
        return json.loads(content)


def readRobust(filename):
    with open(filename, 'r') as openfile:
        return loadRobust(openfile.read())


def printWide(df):
    from itertools import repeat

    mincolwidth = 8
    headerWidths = [ len(f"{c:}") for c in df.columns.to_list() ]
    bodyWidths = df.applymap(lambda s: len("{:}".format(s))).max().to_list()
    colwidths = map(max, zip( headerWidths, bodyWidths, repeat(mincolwidth)))
    colfmts = [ "{:>"+str(w)+"}" for w in colwidths ]

    rowfmt = colfmts[0]
    # Measurements always come in batches of 5
    from itertools import islice
    fmts = iter(colfmts[1:])
    while batch := list(islice(fmts,5)):
        rowfmt += " | " + " ".join(batch)

    print(rowfmt.format(*(df.columns)))
    for index, row in df.fillna("").iterrows():
        print(rowfmt.format(*row.to_list()))

@functools.cache
def ns_to_unit_factor(unit):
    return {
        'ns': 1,
        'us': 1e-3,
        'ms': 1e-6,
        's': 1e-9,
        'm': 1e-9/60,
        'h': 1e-9/3600
    }[unit]

def ns_to_unit(obj, unit):
    factor = ns_to_unit_factor(unit)
    import numbers
    if not isinstance(obj, (numbers.Integral, numbers.Real)):
        import pandas
        if isinstance(obj, (pandas.Timedelta, pandas.Timestamp)):
            return factor * obj.value

    return obj * factor

def alignEvents(events):
    """Aligns passed events of multiple ranks and or participants.
    All ranks of a participant align at initialization, ensured by a barrier in preCICE.
    Primary ranks of all participants align after successfully establishing primary connections.
    """
    assert "events" in events and "eventDict" in events, "Not a loaded event file"
    assert len(events.get("events")) > 0, "No participants in the file"
    participants = events["events"].keys()
    grouped = events["events"]

    # Align ranks of each participant
    for participant, ranks in grouped.items():
        if len(ranks) == 1:
            continue
        print(f"Aligning {len(ranks)} ranks of {participant}")
        intraSyncID = [id for id, name in events["eventDict"].items(
        ) if "com.initializeIntraCom" in name][0]
        syncs = {rank: e["ts"]+e["dur"]
                 for rank, data in ranks.items()
                 for e in data["events"]
                 if e["eid"] == intraSyncID
                 }

        firstSync = min(syncs.values())
        shifts = {rank: firstSync-tp for rank, tp in syncs.items()}

        for rank, data in ranks.items():
            for e in data["events"]:
                e["ts"] += shifts[rank]

    if len(grouped) == 1:
        return events

    # Align participants
    primaries = [name for name, ranks in events["events"].items()
                 if 0 in ranks]

    for lonely in set(events["events"].keys()).difference(primaries):
        print(f"Cannot align {lonely} as event file of rank 0 is missing.")

    # Cannot align anything
    if len(primaries) == 1:
        return events

    # Find synchronization points
    syncEvents = ["m2n.acceptPrimaryRankConnection.",
                  "m2n.requestPrimaryRankConnection."]
    # Event ID -> Remote name
    syncIDs = {id: name.rsplit(".", 1)[1]
               for id, name in events["eventDict"].items()
               if any([check in name for check in syncEvents])
               }
    syncs = {
        local: {
            remote: e["ts"] + e["dur"]
            for e in events["events"][local][0]["events"]
            for eid, remote in syncIDs.items()
            if eid == e["eid"]
        }
        for local in primaries
    }

    def hasSync(l, r): return syncs.get(l) and syncs.get(
        l).get(r) and syncs.get(r) and syncs.get(r).get(l)
    shifts = {
        (local, remote): syncs[local][remote] - syncs[remote][local]

        # all unique participant combinations
        for local in primaries
        for remote in primaries
        if local < remote
        if hasSync(local, remote)
    }
    for (local, remote), shift in shifts.items():
        print(f"Aligning {remote} ({shift}us) with {local}")
        for rank, data in events["events"][remote].items():
            data["meta"]["unix_us"] += shift
            for e in data["events"]:
                e["ts"] += shift

    return events


def groupEvents(events, initTime, nameMapping):
    completed = []
    active = {}  # name to event data

    for event in events:
        type = event["et"]
        if type == "n":
            continue

        name = nameMapping[event["eid"]]
        assert isinstance(name, int)
        # Handle event starts
        if type == "b":
            # assert(name not in active.keys())
            if (name in active.keys()):
                print(f"Ignoring start of active event {name}")
            else:
                event["ts"] = int(event["ts"])
                event["eid"] = name # map to global name
                active[name] = event
        # Handle event stops
        elif type == "e":
            # assert(name in active.keys())
            if (name not in active.keys()):
                print(f"Ignoring end of inactive event {name}")
            else:
                begin = active[name]
                active.pop(name)
                begin["dur"] = int(event["ts"]) - begin["ts"]
                begin["ts"] = int(begin["ts"]) + initTime
                begin.pop("et")
                completed.append(begin)
        # Handle event data
        elif type == "d":
            if (name not in active.keys()):
                print(f"Dropping data event {name} as event isn't yet known.")
            else:
                d = active[name].get("data", {})
                dname = nameMapping[event["dn"]]
                assert isinstance(dname, int)
                d[dname] = int(event["dv"])
                active[name]["data"] = d

    # Handle leftover events in case of a truncated input file
    if active:
        lastTS = min(map(lambda e: e["ts"] + e["dur"], completed))
        for event in active.values():
            name = event["eid"] # This is a global id
            print(f"Truncating event without end {name}")
            begin = active[name]
            begin["ts"] = int(begin["ts"]) + initTime
            begin["dur"] = lastTS - begin["ts"]
            begin.pop("et")
            completed.append(begin)

    return sorted(completed, key=lambda e: e["ts"])


def loadEventFiles(filenames):
    # Load all jsons
    print("Loading event files")
    jsons = []
    for i, fn in enumerate(filenames):
        jsons.append(readRobust(fn))

    # Get all names
    print("Globalizing event names")
    nameIDs = {} # name to global id
    nameMappings = {} # participant -> localid -> globalid
    for i, json in enumerate(jsons):
        name = json["meta"]["name"]
        rank = int(json["meta"]["rank"])
        localNames = { int(e["eid"]): e["en"]
                      for e in json["events"]
                      if e["et"] == "n"}
        for lid, n in localNames.items():
            # get or create a global id for the name
            gid = nameIDs.setdefault(n, len(nameIDs))
            # register the local to global mapping
            nameMappings.setdefault(name, {}).setdefault(rank, {}) .update({lid: gid})

    # Grouping events
    print("Grouping events")
    events = {}
    for i, json in enumerate(jsons):
        name =  json["meta"]["name"]
        rank= int(json["meta"]["rank"])
        mapping = nameMappings[name][rank]
        unix_us = int(json["meta"]["unix_us"])
        events.setdefault(name, {})[rank] = {
            "meta": {
                "name": name,
                "rank": rank,
                "size": int(json["meta"]["size"]),
                "unix_us": unix_us,
                "tinit": json["meta"]["tinit"]
            },
            "events": groupEvents(json["events"], unix_us, mapping)
        }

    return {
        "eventDict":  { id: n for n, id in nameIDs.items()},
        "events": events
    }


class RankData:
    def __init__(self, data):
        meta = data["meta"]
        self.name = meta["name"]
        self.rank = meta["rank"]
        self.size = meta["size"]
        self.unix_us = meta["unix_us"]
        self.tinit = meta["tinit"]

        self.events = data["events"]

    @property
    def type(self):
        return "Primary (0)" if self.rank == 0 else f"Secondary ({self.rank})"

    def toDataFrame(self, eventLookup):
        import pandas
        df = pandas.DataFrame(self.events)
        if "data" in df:
            df.drop("data", axis=1, inplace=True)
        df["dur"] = pandas.to_timedelta(df["dur"], unit='microseconds')
        df["ts"] = pandas.to_datetime(df["ts"], unit="us", origin="unix")
        df.insert(0, 'participant', self.name)
        df.insert(1, 'rank', self.rank)
        df["eid"] = df["eid"].apply(lambda id: eventLookup[id])
        return df

    def toListOfTuples(self, eventLookup):
        for e in self.events:
            yield (self.name, self.rank, eventLookup[e["eid"]], int(e["ts"]), int(e["dur"]))

class Run:
    def __init__(self, filename):
        print(f"Reading events file {filename}")
        import json
        with open(filename, 'r') as f:
            content = json.load(f)

        self.eventLookup = {int(id): name for id,
                            name in content["eventDict"].items()}
        self.data = content["events"]

    def iterRanks(self):
        for pranks in self.data.values():
            for d in sorted(pranks.values(), key=lambda data: int(data["meta"]["rank"])):
                yield RankData(d)

    def iterParticipant(self, name):
        for d in self.data[name].values():
            yield RankData(d)

    def participants(self):
        return self.data.keys()

    def lookupEvent(self, id):
        return self.eventLookup[int(id)]

    def toTrace(self, selectRanks):
        print(f"Selected ranks {selectRanks}")

        def filterop(rank):
            return True if not selectRanks else rank.rank in selectRanks

        pids = {name: id for id, name in enumerate(self.participants())}
        tids = { (rank.name, rank.rank): id
                for id, rank in enumerate(filter(filterop, self.iterRanks()))
                }
        metaEvents = [
            {"name": "process_name", "ph": "M", "pid": pid, "args": {"name": name}}
            for name, pid in pids.items()
        ] + [
            {"name": "thread_name", "ph": "M", "pid": pid,
                "tid": tids[(rank.name, rank.rank)], "args": {"name": rank.type}}
            for name, pid in pids.items()
            for rank in filter(filterop, self.iterParticipant(name))
        ]

        mainEvents = []
        for rank in  filter(filterop, self.iterRanks()):
            pid, tid = pids[rank.name], tids[(rank.name, rank.rank)]
            for e in rank.events:
                en = self.lookupEvent(e["eid"])
                mainEvents.append(
                    mergedDict({
                    "name": en,
                    "cat": "Solver" if en.startswith("solver") else "preCICE",
                    "ph": "X",  # complete event
                    "pid": pid,
                    "tid": tid,
                    "ts": e["ts"],
                    "dur": e["dur"]
                }, {} if "data" not in e else {"args": dict(zip(map(self.lookupEvent, e["data"].keys()), e["data"].values()))}))

        return {"traceEvents": metaEvents + mainEvents}

    def toExportList(self, unit=None):
        factor = ns_to_unit_factor(unit)*1e3 if unit else 1
        for rank in self.iterRanks():
            for e in rank.events:
                yield (
                    rank.name,
                    rank.rank,
                    rank.size,
                    self.lookupEvent(e["eid"]),
                    e["ts"],
                    e["dur"] * factor,
                    "" if "data" not in e else str(dict(zip(map(self.lookupEvent, e["data"].keys()), e["data"].values())))
                )

    def toDataFrame(self):
        import pandas
        import itertools
        columns=["participant", "rank", "eid", "ts", "dur" ]
        df = pandas.DataFrame(itertools.chain.from_iterable(map(lambda r: r.toListOfTuples(self.eventLookup), self.iterRanks())), columns=columns)
        df["dur"] = pandas.to_timedelta(df["dur"], unit='microseconds')
        df["ts"] = pandas.to_datetime(df["ts"], unit="us", origin="unix")
        return df


def traceCommand(eventfile, outfile, rankfilter, limit):
    run = Run(eventfile)
    selection = set().union(rankfilter if rankfilter else []).union(range(limit) if limit else [])
    traces = run.toTrace(selection)
    #json_object = json.dumps(traces, indent=2)
    print(f"Writing to {outfile}")
    with open(outfile, "w") as outfile:
        json.dump(traces, outfile)#, indent=2)
        #outfile.write(json_object)
    return 0


def exportCommand(eventfile, outfile, unit):
    run = Run(eventfile)
    fieldnames = [ "participant", "rank", "size", "event", "timestamp", "duration", "data"]
    print(f"Writing to {outfile}")
    with open(outfile, 'w', newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(fieldnames)
        writer.writerows(run.toExportList(unit))
    return 0


def analyzeCommand(eventfile, participant, outfile=None, unit='us'):
    requireModules(["pandas"])
    import pandas

    run = Run(eventfile)
    df = run.toDataFrame()

    # Filter by participant
    df = df[df["participant"] == participant]
    df.drop("participant", axis=1, inplace=True)

    # Convert duration to requested unit
    df["dur"] = df["dur"].apply(lambda x: x.value * ns_to_unit_factor(unit))
    print(f"Output timing are in {unit}.")

    ranks = df["rank"].unique()

    if len(ranks) == 1:
        primary = df[df["rank"] == 0].groupby("eid").agg(
            {"dur": [("sum", "sum"), ("count", "count"), ("mean", "mean"), ("min", "min"), ("max", "max")]})
        primary.columns = primary.columns.droplevel()
        joined = primary
    else:
        rankAdvance = df[(df["eid"] == "advance") & (df["rank"] > 0)].groupby("rank").sum("dur")
        minSecRank = rankAdvance["dur"].idxmin()
        maxSecRank = rankAdvance["dur"].idxmax()

        print(f"Selection contains the primary rank 0, the cheapest secondary rank {minSecRank}, and the most expensive secondary rank {maxSecRank}.")

        prefix = "R0"
        primary = df[df["rank"] == 0].groupby("eid").agg(
            {"dur": [(f"{prefix}:sum", "sum"), (f"{prefix}:count", "count"), (f"{prefix}:mean", "mean"), (f"{prefix}:min", "min"), (f"{prefix}:max", "max")]})
        primary.columns = primary.columns.droplevel()

        prefix = f"R{minSecRank}"
        minSec = df[df["rank"] == minSecRank].groupby("eid").agg(
            {"dur": [(f"{prefix}:sum", "sum"), (f"{prefix}:count", "count"), (f"{prefix}:mean", "mean"), (f"{prefix}:min", "min"), (f"{prefix}:max", "max")]})
        minSec.columns = minSec.columns.droplevel()

        prefix = f"R{maxSecRank}"
        maxSec = df[df["rank"] == maxSecRank].groupby("eid").agg(
            {"dur": [(f"{prefix}:sum", "sum"), (f"{prefix}:count", "count"), (f"{prefix}:mean", "mean"), (f"{prefix}:min", "min"), (f"{prefix}:max", "max")]})
        maxSec.columns = maxSec.columns.droplevel()

        joined = pandas.concat([primary, minSec, maxSec], axis=1)

    joined.reset_index(inplace=True, names="event")

    printWide(joined)

    if (outfile):
        print(f"Writing to {outfile}")
        joined.to_csv(outfile, index=False)

    return 0

def histogramCommand(eventfile, outfile, participant, event, rank, bins, unit='us'):
    requireModules(["pandas", "matplotlib"])
    import pandas
    import matplotlib.pyplot as plt

    run = Run(eventfile)
    df = run.toDataFrame()

    # Check user input
    assert participant in df["participant"].unique(), f"Given participant {participant} doesn't exist."
    assert event in df["eid"].unique(), f"Given event {event} doesn't exist."
    if not rank is None:
        assert rank in df["rank"].unique(), f"Given rank {rank} doesn't exist."


    # Filter by participant and event
    df = df[(df["participant"] == participant) & (df["eid"] == event)]
    df.drop(["participant", "eid"], axis=1, inplace=True)

    if not rank is None:
        df = df[df["rank"] == rank]

    # Convert duration to requested unit
    df.dur = df.dur.apply(lambda td: ns_to_unit(td, unit))

    durations = df["dur"].to_list()

    ranks = df["rank"].unique()
    rankDesc = "ranks: " + ",".join(map(str,ranks)) if len(ranks) < 5 else f"{len(ranks)} ranks"

    fig, ax = plt.subplots(figsize=(14,7), tight_layout=True)
    ax.set_title(f"Histogram of event \"{event}\" on {participant} ({rankDesc})")
    ax.set_xlabel(f"Duration [{unit}]")
    ax.set_ylabel("Occurrence")
    hist_data = ax.hist(durations, bins=bins, histtype="barstacked", align="mid")
    ax.bar_label(hist_data[2])
    binborders = hist_data[1]
    ax.set_xticks(binborders, labels=[ f"{d:,.2f}" for d in binborders], rotation=90)
    ax.set_xlim(left=min(binborders), right=max(binborders))
    ax.grid(axis="x")

    if (outfile):
        print(f"Writing to {outfile}")
        plt.savefig(outfile)
    else:
        plt.show()

    return 0

def detectFiles(files):
    # Default to loading from the local directory
    if not files:
        files = ["."]

    def searchDir(directory):
        assert os.path.isdir(directory)
        import glob
        return glob.glob(os.path.join(directory, "*-*-*.json"))

    resolved = []
    for path in files:
        if os.path.isfile(path):
            resolved.append(path)
            continue
        if os.path.isdir(path):
            print(f"Searching {path} : ", end="")
            candidates = [".",
                          "precice-events",
                          "../precice-events",
                          "../../precice-events",
                          ]
            found = False
            for candidate in candidates:
                dir = os.path.join(path, candidate)
                if not os.path.isdir(dir):
                    continue
                detected = searchDir(dir)
                if len(detected) == 0:
                    continue
                print(f"found {len(detected)} files in {dir}")
                resolved += detected
                found = True
                break

            if not found:
                print(f"nothing found")

        else:
            print(f"Cannot interpret \"{path}\"")
    unique = list(set(resolved))
    print(f"Found {len(unique)} unique event files")
    return unique


def findFilesOfLatestRun(name, sizes):
    assert len(sizes)>1
    print(f"Found multiple runs for participant {name}")
    timestamps = []
    for size, ranks in sizes.items():
        assert len(ranks) > 0
        example = next(iter(ranks.values())) # Get some file of this run
        timestamp = int(readRobust(example)["meta"]["unix_us"])
        timestamps.append((size, timestamp))

    # Find oldest size of newest timestamps
    size, _ = max(timestamps, key=lambda p: p[1])
    print(f"â”” selected latest run of size {size}")

    return list(sizes[size].values())

def sanitizeFiles(files):
    PieceFile = namedtuple("PieceFile", ["name", "rank", "size", "filename"])
    def info(filename):
        base = os.path.splitext(os.path.basename(filename))[0]
        parts = base.split("-")
        name = "-".join(parts[:-2])
        return PieceFile(name, int(parts[-2]), int(parts[-1]) , filename)

    pieces = [ info(filename) for filename in files ]

    map = {}
    for n, r, s, fn in pieces:
        map.setdefault(n, {}).setdefault(s, {}).update({r: fn})

    filesToLoad = []
    for name, sizes in map.items():
        if len(sizes) == 1:
            print(f"Found a single run for participant {name}")
            filesToLoad += [
                filename
                for _, ranks in sizes.items()
                for filename in ranks.values()
            ]
            continue

        filesToLoad = findFilesOfLatestRun(name, sizes)
    return filesToLoad


def mergeCommand(files, outfile, align):
    resolved = detectFiles(files)
    sanitized = sanitizeFiles(resolved)
    merged = loadEventFiles(sanitized)

    if align:
        merged = alignEvents(merged)

    print(f"Writing to {outfile}")
    with open(outfile, 'w', newline='') as file:
        json.dump(merged, file, separators=(",", ":"))

    return 0


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="", epilog="")
    subparsers = parser.add_subparsers(title='commands',
                                       description='Note that some commands may require pandas.',
                                       help='additional help',
                                       dest="cmd"
                                       )
    # parser.add_argument("-v", "--verbose", help="Print verbose output")
    parser.add_argument("-u", "--unit", choices=["h", "m", "s", "ms", "us"], default="us",
                        help="The duration unit to use")

    analyze_help = """Analyze events data of a given solver.
    Event durations are displayed in the unit of choice.
    Parallel solvers show events of the primary rank next to the secondary ranks spending the least and most time in advance of preCICE.
    """
    analyze = subparsers.add_parser('analyze',
                                    help=analyze_help.splitlines()[0], description=analyze_help)
    analyze.add_argument("participant", type=str,
                         help="The participant to analyze")
    analyze.add_argument("eventfile", nargs="?", type=str, default="events.json",
                         help="The event file to process")
    analyze.add_argument("-o", "--output",
                         help="Write the result to CSV file")

    def try_int(s):
        try:
            return int(s)
        except:
            return s

    histogram_help = """Plots the duration distribution of a single event of a given solver.
    Event durations are displayed in the unit of choice.
    """
    histogram = subparsers.add_parser('histogram',
                                    help=histogram_help.splitlines()[0], description=histogram_help)
    histogram.add_argument("-o", "--output", default=None, help="Write to file instead of displaying the plot")
    histogram.add_argument("-r", "--rank", type=int, default=None, help="Display only the given rank")
    histogram.add_argument("-b", "--bins", type=try_int, default="fd", help="Number of bins or strategy. Must be a valid argument to numpy.histogram_bin_edges")
    histogram.add_argument("participant", type=str, help="The participant to analyze")
    histogram.add_argument("event", type=str, help="The event to analyze")
    histogram.add_argument("eventfile", nargs="?", type=str, default="events.json",
                         help="The event file to process")

    trace_help = "Transform events to the Trace Event Format."
    trace = subparsers.add_parser('trace',
                                  help=trace_help.splitlines()[0], description=trace_help)
    trace.add_argument("eventfile", type=str, nargs="?", default="events.json",
                       help="The event file to process")
    trace.add_argument("-o", "--output", default="trace.json",
                       help="The resulting trace file")
    trace.add_argument("-l", "--limit", type=int, metavar="n", help="Select the first n ranks")
    trace.add_argument("-r", "--rank", type=int, nargs="*",  help="Select individual ranks")

    export_help = "Export the events as a CSV file."
    export = subparsers.add_parser('export',
                                   help=export_help.splitlines()[0], description=export_help)
    export.add_argument("eventfile", nargs="?", type=str, default="events.json",
                        help="The event files to process")
    export.add_argument("-o", "--output", type=str, default="events.csv",
                        help="The CSV file to export to.")

    merge_help = "Transform multiple events to the Trace Event Format."
    merge = subparsers.add_parser('merge',
                                  help=merge_help.splitlines()[0], description=merge_help)
    merge.add_argument("files", nargs="*",
                       help="The event files to process, directories to search, or nothing to autodetect")
    merge.add_argument("-o", "--output", type=str, default="events.json",
                       help="The resulting event file.")
    merge.add_argument("-n", "--no-align", action="store_true", help="Don't align participants?")
    args = parser.parse_args()

    dispatcher = {
        "trace": lambda ns: traceCommand(ns.eventfile, ns.output, ns.rank, ns.limit),
        "export": lambda ns: exportCommand(ns.eventfile, ns.output, ns.unit),
        "analyze": lambda ns: analyzeCommand(ns.eventfile, ns.participant, ns.output,  ns.unit),
        "histogram": lambda ns: histogramCommand(ns.eventfile, ns.output, ns.participant, ns.event, ns.rank, ns.bins, ns.unit),
        "merge": lambda ns: mergeCommand(ns.files, ns.output, not ns.no_align)
    }

    def showHelp(ns):
        parser.print_help()
        return 1

    sys.exit(dispatcher.get(args.cmd, showHelp)(args))
